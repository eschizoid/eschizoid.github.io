<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Introduction to MLOps With SageMaker: Running your First LLM - Mariano Gonzalez</title><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta itemprop="name" content="Introduction to MLOps With SageMaker: Running your First LLM">
<meta itemprop="description" content="Deploying Dolly with SageMaker"><meta itemprop="datePublished" content="2023-04-26T17:27:40-05:00" />
<meta itemprop="dateModified" content="2023-04-26T17:27:40-05:00" />
<meta itemprop="wordCount" content="791">
<meta itemprop="keywords" content="mlops,llm,dolly,langchain,sagemaker,hugging face,aws,databricks,activeloop," /><meta property="og:title" content="Introduction to MLOps With SageMaker: Running your First LLM" />
<meta property="og:description" content="Deploying Dolly with SageMaker" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mariano-gonzalez.com/posts/post-2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-26T17:27:40-05:00" />
<meta property="article:modified_time" content="2023-04-26T17:27:40-05:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Introduction to MLOps With SageMaker: Running your First LLM"/>
<meta name="twitter:description" content="Deploying Dolly with SageMaker"/>
<link href='https://fonts.googleapis.com/css?family=Playfair+Display:700' rel='stylesheet' type='text/css'>
	<link rel="stylesheet" type="text/css" media="screen" href="https://mariano-gonzalez.com/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="https://mariano-gonzalez.com/css/main.css" />
	<script defer type="text/javascript"  src="/js/copy-code.js"></script>

        <link id="dark-scheme" rel="stylesheet" type="text/css" href="https://mariano-gonzalez.com/css/dark.css" />

	<script src="https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js"></script>
		<script src="https://mariano-gonzalez.com/js/main.js"></script>

	
		<link rel="stylesheet" href="/css/copy-code-button.css">
	
</head>

<body>
	<div class="container wrapper">
		<div class="header">
	
	<h1 class="site-title"><a href="https://mariano-gonzalez.com/">Mariano Gonzalez</a></h1>
	<div class="site-description"><p>Coder and Computer Enthusiast</p><nav class="nav social">
			<ul class="flat"><li><a href="https://www.linkedin.com/in/marianogonzalezmx/?locale=en_US" title="LinkedIn"><i data-feather="linkedin"></i></a></li><li><a href="https://github.com/eschizoid" title="Github"><i data-feather="github"></i></a></li><li><a href="/index.xml" title="RSS"><i data-feather="rss"></i></a></li><li><a href="#" class="scheme-toggle" id="scheme-toggle"></a></li></ul>
		</nav>
	</div>

	<nav class="nav">
		<ul class="flat">
			
			<li>
				<a href="/">Home</a>
			</li>
			
			<li>
				<a href="/posts">All posts</a>
			</li>
			
			<li>
				<a href="/about">About</a>
			</li>
			
			<li>
				<a href="/tags">Tags</a>
			</li>
			
		</ul>
	</nav>
</div>


		<div class="post">
			<div class="post-header">
				
					<div class="meta">
						<div class="date">
							<span class="day">26</span>
							<span class="rest">Apr 2023</span>
						</div>
					</div>
				
				<div class="matter">
					<h1 class="title">Introduction to MLOps With SageMaker: Running your First LLM</h1>
				</div>
			</div>
					
			<div class="markdown">
				<h2 id="introduction">Introduction</h2>
<p>As the field of machine learning advances, it has become increasingly important for organizations to develop robust
practices for managing their workflows. That&rsquo;s where MLOps comes in - a set of best practices and tools for managing the
entire lifecycle of machine learning models, from development to deployment and beyond.</p>
<p>In this blog post, we&rsquo;ll delve into how MLOps practices can be leveraged to deploy an LLM
in <a href="https://aws.amazon.com/sagemaker/">AWS SageMaker</a>, using the
popular <a href="https://github.com/aws/sagemaker-huggingface-inference-toolkit">Hugging Face Transformers library</a>. We&rsquo;ll cover
everything from setting up an end-to-end pipeline for deploying a Large Language Model on SageMaker, to monitoring its
performance.</p>
<p>By the end of this post, you&rsquo;ll have a better understanding of the key components of an MLOps workflow, and how they can
be used to streamline the deployment of complex machine learning models in production environments. Whether you&rsquo;re an
experienced machine learning practitioner or just starting out, this post will provide valuable insights into the
cutting-edge tools and techniques driving the field forward.</p>
<h2 id="getting-started">Getting Started</h2>
<h3 id="prerequisites---runtimes">Prerequisites - Runtimes</h3>
<p>Install the following binaries on your machine:</p>
<figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>brew install awscli
</span></span><span style="display:flex;"><span>brew install go-task
</span></span><span style="display:flex;"><span>brew install terraform</span></span></code></pre>
</figure><h3 id="prerequisites---aws-resources">Prerequisites - AWS Resources</h3>
<p>Make sure your have an AWS account configured:</p>
<figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>cat ~/.aws/config
</span></span><span style="display:flex;"><span><span style="color:#79c0ff">aws_access_key_id</span> <span style="color:#ff7b72;font-weight:bold">=</span> <span style="color:#ff7b72;font-weight:bold">[</span>REDACTED<span style="color:#ff7b72;font-weight:bold">]</span>
</span></span><span style="display:flex;"><span><span style="color:#79c0ff">aws_secret_access_key</span> <span style="color:#ff7b72;font-weight:bold">=</span> <span style="color:#ff7b72;font-weight:bold">[</span>REDACTED<span style="color:#ff7b72;font-weight:bold">]</span></span></span></code></pre>
</figure><ol>
<li>Clone the repo:
<figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>git clone https://github.com/eschizoid/secon-2023.git</span></span></code></pre>
</figure></li>
<li>Run terraform init to check the provider loaded as expected:
<figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>task tf_init</span></span></code></pre>
</figure></li>
<li>Run terraform plan
<figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>task tf_plan</span></span></code></pre>
</figure></li>
<li>Create SageMaker domain, user profile, and JupyterServer instance:
<figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>task tf_apply</span></span></code></pre>
</figure></li>
</ol>
<h3 id="creating-modeltargz-file">Creating <code>model.tar.gz</code> file</h3>
<ol>
<li>
<p>The first step is to create a folder structure like the following:</p>
<figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>model.tar.gz/
</span></span><span style="display:flex;"><span>|- model/code/
</span></span><span style="display:flex;"><span>  |- inference.py
</span></span><span style="display:flex;"><span>  |- requirements.txt   </span></span></code></pre>
</figure></li>
<li>
<p>Using the <a href="https://github.com/aws/sagemaker-huggingface-inference-toolkit">SageMaker Hugging Face Inference Toolkit</a>,
we can reference <a href="https://huggingface.co/databricks/dolly-v2-12b">Dolly</a> in SageMaker by creating a function like the
one below in the file <code>inference.py</code>. By doing this we will be overwriting the <code>model_fn</code> function:</p>
<figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff7b72">import</span> <span style="color:#ff7b72">torch</span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">from</span> <span style="color:#ff7b72">transformers</span> <span style="color:#ff7b72">import</span> pipeline
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">def</span> <span style="color:#d2a8ff;font-weight:bold">model_fn</span>(model_dir):
</span></span><span style="display:flex;"><span>    instruct_pipeline <span style="color:#ff7b72;font-weight:bold">=</span> pipeline(
</span></span><span style="display:flex;"><span>        model<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;databricks/dolly-v2-12b&#34;</span>,
</span></span><span style="display:flex;"><span>        torch_dtype<span style="color:#ff7b72;font-weight:bold">=</span>torch<span style="color:#ff7b72;font-weight:bold">.</span>bfloat16,
</span></span><span style="display:flex;"><span>        trust_remote_code<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#79c0ff">True</span>,
</span></span><span style="display:flex;"><span>        device_map<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;auto&#34;</span>,
</span></span><span style="display:flex;"><span>        model_kwargs<span style="color:#ff7b72;font-weight:bold">=</span>{<span style="color:#a5d6ff">&#34;load_in_8bit&#34;</span>: <span style="color:#79c0ff">True</span>},
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    <span style="color:#ff7b72">return</span> instruct_pipeline</span></span></code></pre>
</figure></li>
<li>
<p>Finally, upload model to S3:</p>
<figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>task tar_model
</span></span><span style="display:flex;"><span>task upload_model</span></span></code></pre>
</figure></li>
</ol>
<h2 id="provisioning-jupyterserver">Provisioning JupyterServer</h2>
<p>Once the infrastructure is up and running and the model reference has been uploaded to S3, you can access the
JupyterServer by clicking the button &ldquo;Open Studio&rdquo; in the SageMaker console:</p>
<p><p class="md__image">
    <img src="images/jupyterserver-launch.png" alt=""  />
</p></p>
<h2 id="deploying-llm---dolly-v2-12b">Deploying LLM - Dolly V2 12B</h2>
<p>From the JupyterServer, you can import the <a href="https://github.com/eschizoid/secon-2023">git</a> repo and reference the
notebook <code>notebooks/deploy-to-sm-endpoint.ipynb</code>.</p>
<p>After executing all the previous cells from the notebook, you can proceed to deploy the model like the image below
shows:</p>
<p><p class="md__image">
    <img src="images/jupyterserver-endpoint-1.png" alt=""  />
</p></p>
<hr>
<p><strong>NOTE</strong></p>
<p>It&rsquo;s important to mention that this step might take a couple of minutes to complete.</p>
<hr>
<p>You can verify the model was deployed successfully by checking the SageMaker endpoint status on the AWS Console:</p>
<p><p class="md__image">
    <img src="images/jupyterserver-endpoint-2.png" alt=""  />
</p></p>
<h2 id="consuming-sagemaker-endpoint">Consuming SageMaker Endpoint</h2>
<p>We can use the <a href="https://streamlit.io">Streamlit</a> to create a fast application and test the model inference:</p>
<figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff7b72">import</span> <span style="color:#ff7b72">json</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">import</span> <span style="color:#ff7b72">boto3</span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">import</span> <span style="color:#ff7b72">streamlit</span> <span style="color:#ff7b72">as</span> <span style="color:#ff7b72">st</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">def</span> <span style="color:#d2a8ff;font-weight:bold">generate_text</span>(input_prompt: str) <span style="color:#ff7b72;font-weight:bold">-&gt;</span> str:
</span></span><span style="display:flex;"><span>    payload <span style="color:#ff7b72;font-weight:bold">=</span> {
</span></span><span style="display:flex;"><span>        <span style="color:#a5d6ff">&#34;inputs&#34;</span>: input_prompt,
</span></span><span style="display:flex;"><span>        <span style="color:#a5d6ff">&#34;min_length&#34;</span>: min_length,
</span></span><span style="display:flex;"><span>        <span style="color:#a5d6ff">&#34;max_length&#34;</span>: max_length,
</span></span><span style="display:flex;"><span>        <span style="color:#a5d6ff">&#34;temperature&#34;</span>: temperature,
</span></span><span style="display:flex;"><span>        <span style="color:#a5d6ff">&#34;repetition_penalty&#34;</span>: rep_penalty,
</span></span><span style="display:flex;"><span>        <span style="color:#a5d6ff">&#34;do_sample&#34;</span>: temperature <span style="color:#ff7b72;font-weight:bold">&gt;</span> <span style="color:#a5d6ff">0</span>,
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    response <span style="color:#ff7b72;font-weight:bold">=</span> sagemaker_runtime<span style="color:#ff7b72;font-weight:bold">.</span>invoke_endpoint(
</span></span><span style="display:flex;"><span>        EndpointName<span style="color:#ff7b72;font-weight:bold">=</span>endpoint_name,
</span></span><span style="display:flex;"><span>        ContentType<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;application/json&#34;</span>,
</span></span><span style="display:flex;"><span>        Body<span style="color:#ff7b72;font-weight:bold">=</span>json<span style="color:#ff7b72;font-weight:bold">.</span>dumps(payload)
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    result <span style="color:#ff7b72;font-weight:bold">=</span> json<span style="color:#ff7b72;font-weight:bold">.</span>loads(response[<span style="color:#a5d6ff">&#34;Body&#34;</span>]<span style="color:#ff7b72;font-weight:bold">.</span>read()<span style="color:#ff7b72;font-weight:bold">.</span>decode())
</span></span><span style="display:flex;"><span>    <span style="color:#ff7b72">return</span> result[<span style="color:#a5d6ff">0</span>][<span style="color:#a5d6ff">&#34;generated_text&#34;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>session <span style="color:#ff7b72;font-weight:bold">=</span> boto3<span style="color:#ff7b72;font-weight:bold">.</span>Session()
</span></span><span style="display:flex;"><span>sagemaker_runtime <span style="color:#ff7b72;font-weight:bold">=</span> session<span style="color:#ff7b72;font-weight:bold">.</span>client(<span style="color:#a5d6ff">&#34;sagemaker-runtime&#34;</span>, region_name<span style="color:#ff7b72;font-weight:bold">=</span>session<span style="color:#ff7b72;font-weight:bold">.</span>region_name)
</span></span><span style="display:flex;"><span>endpoint_name <span style="color:#ff7b72;font-weight:bold">=</span> <span style="color:#a5d6ff">&#34;dolly-v2-12b&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>st<span style="color:#ff7b72;font-weight:bold">.</span>sidebar<span style="color:#ff7b72;font-weight:bold">.</span>title(<span style="color:#a5d6ff">&#34;Dolly-V2 Parameters&#34;</span>)
</span></span><span style="display:flex;"><span>stop_word <span style="color:#ff7b72;font-weight:bold">=</span> st<span style="color:#ff7b72;font-weight:bold">.</span>sidebar<span style="color:#ff7b72;font-weight:bold">.</span>text_input(<span style="color:#a5d6ff">&#34;Stop word&#34;</span>)
</span></span><span style="display:flex;"><span>min_length, max_length <span style="color:#ff7b72;font-weight:bold">=</span> st<span style="color:#ff7b72;font-weight:bold">.</span>sidebar<span style="color:#ff7b72;font-weight:bold">.</span>slider(<span style="color:#a5d6ff">&#34;Min/Max length&#34;</span>, <span style="color:#a5d6ff">0</span>, <span style="color:#a5d6ff">500</span>, (<span style="color:#a5d6ff">0</span>, <span style="color:#a5d6ff">100</span>))
</span></span><span style="display:flex;"><span>temperature <span style="color:#ff7b72;font-weight:bold">=</span> st<span style="color:#ff7b72;font-weight:bold">.</span>sidebar<span style="color:#ff7b72;font-weight:bold">.</span>slider(<span style="color:#a5d6ff">&#34;Temperature&#34;</span>, min_value<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">0.0</span>, max_value<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">1.0</span>, value<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">0.6</span>)
</span></span><span style="display:flex;"><span>rep_penalty <span style="color:#ff7b72;font-weight:bold">=</span> st<span style="color:#ff7b72;font-weight:bold">.</span>sidebar<span style="color:#ff7b72;font-weight:bold">.</span>slider(<span style="color:#a5d6ff">&#34;Repetition Penalty&#34;</span>, min_value<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">0.9</span>, max_value<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">1.2</span>, value<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">1.0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>st<span style="color:#ff7b72;font-weight:bold">.</span>header(<span style="color:#a5d6ff">&#34;Dolly-v2-12B Playground&#34;</span>)
</span></span><span style="display:flex;"><span>prompt <span style="color:#ff7b72;font-weight:bold">=</span> st<span style="color:#ff7b72;font-weight:bold">.</span>text_area(<span style="color:#a5d6ff">&#34;Enter your prompt here:&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">if</span> st<span style="color:#ff7b72;font-weight:bold">.</span>button(<span style="color:#a5d6ff">&#34;Run&#34;</span>):
</span></span><span style="display:flex;"><span>    generated_text <span style="color:#ff7b72;font-weight:bold">=</span> generate_text(prompt)
</span></span><span style="display:flex;"><span>    <span style="color:#ff7b72">if</span> len(stop_word) <span style="color:#ff7b72;font-weight:bold">&gt;</span> <span style="color:#a5d6ff">0</span>:
</span></span><span style="display:flex;"><span>        generated_text <span style="color:#ff7b72;font-weight:bold">=</span> generated_text[:generated_text<span style="color:#ff7b72;font-weight:bold">.</span>rfind(stop_word)]
</span></span><span style="display:flex;"><span>    st<span style="color:#ff7b72;font-weight:bold">.</span>write(generated_text)</span></span></code></pre>
</figure><p>And finally execute the following command to deploy using docker:</p>
<figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>task run_playground</span></span></code></pre>
</figure><p><p class="md__image">
    <img src="images/playground-ui.png" alt=""  />
</p></p>
<p>You will be able to access the playground on: <code>http://localhost/8501/</code></p>
<h2 id="bonus-more-sophisticated-workflows">Bonus: More sophisticated workflows</h2>
<p>You can build need more sophisticated workflows by templating the prompts
using <a href="https://python.langchain.com/en/latest/">langchain</a>. The following are just a few examples of what you can do by
combining <code>langchain</code> and <code>SageMaker</code>:</p>
<figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>!pip install langchain
</span></span><span style="display:flex;"><span>!aws configure set aws_access_key_id <span style="color:#ff7b72;font-weight:bold">[</span>REDACTED<span style="color:#ff7b72;font-weight:bold">]</span>
</span></span><span style="display:flex;"><span>!aws configure set aws_secret_access_key <span style="color:#ff7b72;font-weight:bold">[</span>REDACTED<span style="color:#ff7b72;font-weight:bold">]</span>
</span></span><span style="display:flex;"><span>!aws configure set default.region us-east-1</span></span></code></pre>
</figure><figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff7b72">import</span> <span style="color:#ff7b72">json</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">from</span> <span style="color:#ff7b72">langchain</span> <span style="color:#ff7b72">import</span> SagemakerEndpoint
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">from</span> <span style="color:#ff7b72">langchain.llms.sagemaker_endpoint</span> <span style="color:#ff7b72">import</span> LLMContentHandler
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">class</span> <span style="color:#f0883e;font-weight:bold">ContentHandler</span>(LLMContentHandler):
</span></span><span style="display:flex;"><span>    content_type <span style="color:#ff7b72;font-weight:bold">=</span> <span style="color:#a5d6ff">&#34;application/json&#34;</span>
</span></span><span style="display:flex;"><span>    accepts <span style="color:#ff7b72;font-weight:bold">=</span> <span style="color:#a5d6ff">&#34;application/json&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#ff7b72">def</span> <span style="color:#d2a8ff;font-weight:bold">transform_input</span>(self, prompt: str, model_kwargs) <span style="color:#ff7b72;font-weight:bold">-&gt;</span> bytes:
</span></span><span style="display:flex;"><span>        input_str <span style="color:#ff7b72;font-weight:bold">=</span> json<span style="color:#ff7b72;font-weight:bold">.</span>dumps({prompt: prompt, <span style="color:#ff7b72;font-weight:bold">**</span>model_kwargs})
</span></span><span style="display:flex;"><span>        <span style="color:#ff7b72">return</span> input_str<span style="color:#ff7b72;font-weight:bold">.</span>encode(<span style="color:#a5d6ff">&#39;utf-8&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#ff7b72">def</span> <span style="color:#d2a8ff;font-weight:bold">transform_output</span>(self, output: bytes) <span style="color:#ff7b72;font-weight:bold">-&gt;</span> str:
</span></span><span style="display:flex;"><span>        response_json <span style="color:#ff7b72;font-weight:bold">=</span> json<span style="color:#ff7b72;font-weight:bold">.</span>loads(output<span style="color:#ff7b72;font-weight:bold">.</span>read()<span style="color:#ff7b72;font-weight:bold">.</span>decode(<span style="color:#a5d6ff">&#34;utf-8&#34;</span>))
</span></span><span style="display:flex;"><span>        <span style="color:#ff7b72">return</span> response_json[<span style="color:#a5d6ff">0</span>][<span style="color:#a5d6ff">&#34;generated_text&#34;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>se <span style="color:#ff7b72;font-weight:bold">=</span> SagemakerEndpoint(
</span></span><span style="display:flex;"><span>    endpoint_name<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;dolly-v2-12b&#34;</span>,
</span></span><span style="display:flex;"><span>    region_name<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;us-east-1&#34;</span>,
</span></span><span style="display:flex;"><span>    credentials_profile_name<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;default&#34;</span>,
</span></span><span style="display:flex;"><span>    content_handler<span style="color:#ff7b72;font-weight:bold">=</span>ContentHandler(),
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>se(<span style="color:#a5d6ff">&#34;Tell me a joke&#34;</span>)</span></span></code></pre>
</figure><figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff7b72">from</span> <span style="color:#ff7b72">langchain</span> <span style="color:#ff7b72">import</span> PromptTemplate, LLMChain
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>prompt_template <span style="color:#ff7b72;font-weight:bold">=</span> <span style="color:#a5d6ff">&#34;Why is </span><span style="color:#a5d6ff">{vegetable}</span><span style="color:#a5d6ff"> good for you?&#34;</span>
</span></span><span style="display:flex;"><span>llm_chain <span style="color:#ff7b72;font-weight:bold">=</span> LLMChain(llm<span style="color:#ff7b72;font-weight:bold">=</span>se, prompt<span style="color:#ff7b72;font-weight:bold">=</span>PromptTemplate<span style="color:#ff7b72;font-weight:bold">.</span>from_template(prompt_template))
</span></span><span style="display:flex;"><span>llm_chain(<span style="color:#a5d6ff">&#34;brocolli&#34;</span>)[<span style="color:#a5d6ff">&#34;text&#34;</span>]</span></span></code></pre>
</figure><figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff7b72">import</span> <span style="color:#ff7b72">time</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">from</span> <span style="color:#ff7b72">langchain.chains</span> <span style="color:#ff7b72">import</span> LLMChain
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">from</span> <span style="color:#ff7b72">langchain.prompts</span> <span style="color:#ff7b72">import</span> PromptTemplate
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">def</span> <span style="color:#d2a8ff;font-weight:bold">generate_serially</span>():
</span></span><span style="display:flex;"><span>    prompt <span style="color:#ff7b72;font-weight:bold">=</span> PromptTemplate(
</span></span><span style="display:flex;"><span>        input_variables<span style="color:#ff7b72;font-weight:bold">=</span>[<span style="color:#a5d6ff">&#34;product&#34;</span>],
</span></span><span style="display:flex;"><span>        template<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;What is a good name for a company that makes </span><span style="color:#a5d6ff">{product}</span><span style="color:#a5d6ff">?&#34;</span>,
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    chain <span style="color:#ff7b72;font-weight:bold">=</span> LLMChain(llm<span style="color:#ff7b72;font-weight:bold">=</span>se, prompt<span style="color:#ff7b72;font-weight:bold">=</span>prompt)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">for</span> _ <span style="color:#ff7b72;font-weight:bold">in</span> range(<span style="color:#a5d6ff">5</span>):
</span></span><span style="display:flex;"><span>    resp <span style="color:#ff7b72;font-weight:bold">=</span> chain<span style="color:#ff7b72;font-weight:bold">.</span>run(product<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;underwear&#34;</span>)
</span></span><span style="display:flex;"><span>    print(resp)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>s <span style="color:#ff7b72;font-weight:bold">=</span> time<span style="color:#ff7b72;font-weight:bold">.</span>perf_counter()
</span></span><span style="display:flex;"><span>generate_serially()
</span></span><span style="display:flex;"><span>elapsed <span style="color:#ff7b72;font-weight:bold">=</span> time<span style="color:#ff7b72;font-weight:bold">.</span>perf_counter() <span style="color:#ff7b72;font-weight:bold">-</span> s
</span></span><span style="display:flex;"><span>print(<span style="color:#a5d6ff">&#39;</span><span style="color:#79c0ff">\033</span><span style="color:#a5d6ff">[1m&#39;</span> <span style="color:#ff7b72;font-weight:bold">+</span> <span style="color:#79c0ff">f</span><span style="color:#a5d6ff">&#34;Serial executed in </span><span style="color:#a5d6ff">{</span>elapsed<span style="color:#a5d6ff">:</span><span style="color:#a5d6ff">0.2f</span><span style="color:#a5d6ff">}</span><span style="color:#a5d6ff"> seconds.&#34;</span> <span style="color:#ff7b72;font-weight:bold">+</span> <span style="color:#a5d6ff">&#39;</span><span style="color:#79c0ff">\033</span><span style="color:#a5d6ff">[0m&#39;</span>)</span></span></code></pre>
</figure><figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-python" data-lang="python"><span style="display:flex;"><span>prompt_template <span style="color:#ff7b72;font-weight:bold">=</span> <span style="color:#a5d6ff">&#34;Tell me a </span><span style="color:#a5d6ff">{adjective}</span><span style="color:#a5d6ff"> joke&#34;</span>
</span></span><span style="display:flex;"><span>llm_chain <span style="color:#ff7b72;font-weight:bold">=</span> LLMChain(
</span></span><span style="display:flex;"><span>   llm<span style="color:#ff7b72;font-weight:bold">=</span>se,
</span></span><span style="display:flex;"><span>   prompt<span style="color:#ff7b72;font-weight:bold">=</span>PromptTemplate<span style="color:#ff7b72;font-weight:bold">.</span>from_template(prompt_template)
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>llm_chain(inputs<span style="color:#ff7b72;font-weight:bold">=</span>{<span style="color:#a5d6ff">&#34;adjective&#34;</span>: <span style="color:#a5d6ff">&#34;corny&#34;</span>})</span></span></code></pre>
</figure><h3 id="cleaning-up-resources">Cleaning up resources</h3>
<p>To clean up the resources created by this project, you can run the following command:</p>
<figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>task tf_destroy</span></span></code></pre>
</figure>
			</div>

			<div class="tags">
				
					
						<ul class="flat">
							
							<li><a href="/tags/mlops">mlops</a></li>
							
							<li><a href="/tags/llm">llm</a></li>
							
							<li><a href="/tags/dolly">dolly</a></li>
							
							<li><a href="/tags/langchain">langchain</a></li>
							
							<li><a href="/tags/sagemaker">sagemaker</a></li>
							
							<li><a href="/tags/hugging-face">hugging face</a></li>
							
							<li><a href="/tags/aws">aws</a></li>
							
							<li><a href="/tags/databricks">databricks</a></li>
							
							<li><a href="/tags/activeloop">activeloop</a></li>
							
						</ul>
					
				
			</div><l></l><div id="disqus_thread"></div>
<script>

    (function() {
        
        
        if (window.location.hostname == "localhost")
            return;

        var d = document, s = d.createElement('script');
        s.src = 'https://mariano-gonzalez.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
	</div>
	<div class="footer wrapper">
	<nav class="nav">
		<div>2023  <p>Copyright Â© 2023 Mariano Gonzalez</p><p xmlns:dct='http://purl.org/dc/terms/' xmlns:cc='http://creativecommons.org/ns#' class='license-text'><a rel='cc:attributionURL' property='dct:title' href='https://github.com/eschizoid/eschizoid.github.io/tree/main/content'>Content</a> licensed under <a rel='license' href='https://creativecommons.org/licenses/by/4.0'>CC BY 4.0</a></p> |  <a href="https://github.com/knadh/hugo-ink">Ink</a> theme on <a href="https://gohugo.io">Hugo</a></div>
	</nav>
</div>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-1R50XCY25D"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-1R50XCY25D');
</script><script>feather.replace()</script>
</body>
</html>
