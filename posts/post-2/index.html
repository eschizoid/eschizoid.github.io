<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Introduction to MLOps With SageMaker: Running your First LLM - Mariano Gonzalez</title><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta itemprop="name" content="Introduction to MLOps With SageMaker: Running your First LLM">
<meta itemprop="description" content="Deploying an LLM with SageMaker"><meta itemprop="datePublished" content="2023-04-26T17:27:40-05:00" />
<meta itemprop="dateModified" content="2023-04-26T17:27:40-05:00" />
<meta itemprop="wordCount" content="856">
<meta itemprop="keywords" content="mlops,llm,dolly,langchain,sagemaker,hugging face,aws,databricks,activeloop," /><meta property="og:title" content="Introduction to MLOps With SageMaker: Running your First LLM" />
<meta property="og:description" content="Deploying an LLM with SageMaker" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mariano-gonzalez.com/posts/post-2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-26T17:27:40-05:00" />
<meta property="article:modified_time" content="2023-04-26T17:27:40-05:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Introduction to MLOps With SageMaker: Running your First LLM"/>
<meta name="twitter:description" content="Deploying an LLM with SageMaker"/>
<link href='https://fonts.googleapis.com/css?family=Playfair+Display:700' rel='stylesheet' type='text/css'>
	<link rel="stylesheet" type="text/css" media="screen" href="https://mariano-gonzalez.com/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="https://mariano-gonzalez.com/css/main.css" />
	<script defer type="text/javascript"  src="/js/copy-code.js"></script>

        <link id="dark-scheme" rel="stylesheet" type="text/css" href="https://mariano-gonzalez.com/css/dark.css" />

	<script src="https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js"></script>
		<script src="https://mariano-gonzalez.com/js/main.js"></script>

	
		<link rel="stylesheet" href="/css/copy-code-button.css">
	
</head>

<body>
	<div class="container wrapper">
		<div class="header">
	
	<h1 class="site-title"><a href="https://mariano-gonzalez.com/">Mariano Gonzalez</a></h1>
	<div class="site-description"><p>Coder and Computer Enthusiast</p><nav class="nav social">
			<ul class="flat"><li><a href="https://www.linkedin.com/in/marianogonzalezmx/?locale=en_US" title="LinkedIn"><i data-feather="linkedin"></i></a></li><li><a href="https://github.com/eschizoid" title="Github"><i data-feather="github"></i></a></li><li><a href="/index.xml" title="RSS"><i data-feather="rss"></i></a></li><li><a href="#" class="scheme-toggle" id="scheme-toggle"></a></li></ul>
		</nav>
	</div>

	<nav class="nav">
		<ul class="flat">
			
			<li>
				<a href="/">Home</a>
			</li>
			
			<li>
				<a href="/posts">All posts</a>
			</li>
			
			<li>
				<a href="/about">About</a>
			</li>
			
			<li>
				<a href="/tags">Tags</a>
			</li>
			
		</ul>
	</nav>
</div>


		<div class="post">
			<div class="post-header">
				
					<div class="meta">
						<div class="date">
							<span class="day">26</span>
							<span class="rest">Apr 2023</span>
						</div>
					</div>
				
				<div class="matter">
					<h1 class="title">Introduction to MLOps With SageMaker: Running your First LLM</h1>
				</div>
			</div>
					
			<div class="markdown">
				<h2 id="introduction">Introduction</h2>
<p>As the field of machine learning advances, it has become increasingly important for organizations to develop robust
practices for managing their workflows. That&rsquo;s where MLOps comes in - a set of best practices and tools for managing the
entire lifecycle of machine learning models, from development to deployment and beyond.</p>
<p>In this blog post, we&rsquo;ll delve into how MLOps practices can be leveraged to deploy an LLM
in <a href="https://aws.amazon.com/sagemaker/">AWS SageMaker</a>, using the
popular <a href="https://github.com/aws/sagemaker-huggingface-inference-toolkit">Hugging Face Transformers library</a>. We&rsquo;ll cover
everything from setting up an end-to-end pipeline for deploying a Large Language Model on SageMaker, to monitoring its
performance.</p>
<p>By the end of this post, you&rsquo;ll have a better understanding of the key components of an MLOps workflow, and how they can
be used to streamline the deployment of complex machine learning models in production environments. Whether you&rsquo;re an
experienced machine learning practitioner or just starting out, this post will provide valuable insights into the
cutting-edge tools and techniques driving the field forward.</p>
<h2 id="getting-started">Getting Started</h2>
<h3 id="prerequisites---runtimes">Prerequisites - Runtimes</h3>
<p>Install the following binaries on your machine:</p>
<figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>brew install awscli
</span></span><span style="display:flex;"><span>brew install go-task
</span></span><span style="display:flex;"><span>brew install terraform</span></span></code></pre>
</figure><h3 id="prerequisites---aws-resources">Prerequisites - AWS Resources</h3>
<p>Make sure your have an AWS account configured:</p>
<figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>cat ~/.aws/config
</span></span><span style="display:flex;"><span><span style="color:#79c0ff">aws_access_key_id</span> <span style="color:#ff7b72;font-weight:bold">=</span> <span style="color:#ff7b72;font-weight:bold">[</span>REDACTED<span style="color:#ff7b72;font-weight:bold">]</span>
</span></span><span style="display:flex;"><span><span style="color:#79c0ff">aws_secret_access_key</span> <span style="color:#ff7b72;font-weight:bold">=</span> <span style="color:#ff7b72;font-weight:bold">[</span>REDACTED<span style="color:#ff7b72;font-weight:bold">]</span></span></span></code></pre>
</figure><ol>
<li>Clone the repo:
<figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>git clone https://github.com/eschizoid/secon-2023.git</span></span></code></pre>
</figure></li>
<li>Run terraform init to check the provider loaded as expected:
<figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>task tf_init</span></span></code></pre>
</figure></li>
<li>Run terraform plan
<figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>task tf_plan</span></span></code></pre>
</figure></li>
<li>Create SageMaker domain, user profile, and JupyterServer instance:
<figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>task tf_apply</span></span></code></pre>
</figure></li>
</ol>
<h3 id="creating-modeltargz-for-the-amazon-sagemaker-real-time-endpoint">Creating <code>model.tar.gz</code> for the Amazon SageMaker real-time endpoint</h3>
<ol>
<li>
<p>There are two ways you can deploy transformers to Amazon SageMaker. You can either deploy a model from the Hugging
Face Hub directly or deploy a model stored on S3. Since we are not using the default Transformers method we need to
go with the second option and deploy our endpoint with the model stored on S3. In order to do that we need to create
a folder structure like the following:</p>
<figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>model.tar.gz/
</span></span><span style="display:flex;"><span>|- model/code/
</span></span><span style="display:flex;"><span>  |- inference.py
</span></span><span style="display:flex;"><span>  |- requirements.txt   </span></span></code></pre>
</figure><p>Using the <a href="https://github.com/aws/sagemaker-huggingface-inference-toolkit">SageMaker Hugging Face Inference Toolkit</a>,
we can reference <a href="https://huggingface.co/databricks/dolly-v2-12b">Dolly</a> in SageMaker by creating a function like the
one below in the file <code>inference.py</code>. By doing this we will be overwriting the <code>model_fn</code> function:</p>
<figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff7b72">import</span> <span style="color:#ff7b72">torch</span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">from</span> <span style="color:#ff7b72">transformers</span> <span style="color:#ff7b72">import</span> pipeline
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">def</span> <span style="color:#d2a8ff;font-weight:bold">model_fn</span>(model_dir):
</span></span><span style="display:flex;"><span>    instruct_pipeline <span style="color:#ff7b72;font-weight:bold">=</span> pipeline(
</span></span><span style="display:flex;"><span>        model<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;databricks/dolly-v2-12b&#34;</span>,
</span></span><span style="display:flex;"><span>        torch_dtype<span style="color:#ff7b72;font-weight:bold">=</span>torch<span style="color:#ff7b72;font-weight:bold">.</span>bfloat16,
</span></span><span style="display:flex;"><span>        trust_remote_code<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#79c0ff">True</span>,
</span></span><span style="display:flex;"><span>        device_map<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;auto&#34;</span>,
</span></span><span style="display:flex;"><span>        model_kwargs<span style="color:#ff7b72;font-weight:bold">=</span>{<span style="color:#a5d6ff">&#34;load_in_8bit&#34;</span>: <span style="color:#79c0ff">True</span>},
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    <span style="color:#ff7b72">return</span> instruct_pipeline</span></span></code></pre>
</figure></li>
<li>
<p>Finally, upload model to S3:</p>
<figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>task tar_model
</span></span><span style="display:flex;"><span>task upload_model</span></span></code></pre>
</figure></li>
</ol>
<h2 id="provisioning-jupyterserver">Provisioning JupyterServer</h2>
<p>Once the infrastructure is up and running and the model reference has been uploaded to S3, you can access the
JupyterServer by clicking the button &ldquo;Open Studio&rdquo; in the SageMaker console:</p>
<p><p class="md__image">
    <img src="images/jupyterserver-launch.png" alt=""  />
</p></p>
<h2 id="deploying-llm---dolly-v2-12b">Deploying LLM - Dolly V2 12B</h2>
<p>From the JupyterServer, you can import the <a href="https://github.com/eschizoid/secon-2023">git</a> repo and reference the
notebook <code>notebooks/deploy-to-sm-endpoint.ipynb</code>.</p>
<p>After executing all the previous cells from the notebook, you can proceed to deploy the model like the image below
shows:</p>
<p><p class="md__image">
    <img src="images/jupyterserver-endpoint-1.png" alt=""  />
</p></p>
<hr>
<p><strong>NOTE</strong></p>
<p>It&rsquo;s important to mention that this step might take a couple of minutes to complete.</p>
<hr>
<p>You can verify the model was deployed successfully by checking the SageMaker endpoint status on the AWS Console:</p>
<p><p class="md__image">
    <img src="images/jupyterserver-endpoint-2.png" alt=""  />
</p></p>
<h2 id="consuming-sagemaker-endpoint">Consuming SageMaker Endpoint</h2>
<p>We can use the <a href="https://streamlit.io">Streamlit</a> to create a fast application and test the model inference:</p>
<figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff7b72">import</span> <span style="color:#ff7b72">json</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">import</span> <span style="color:#ff7b72">boto3</span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">import</span> <span style="color:#ff7b72">streamlit</span> <span style="color:#ff7b72">as</span> <span style="color:#ff7b72">st</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">def</span> <span style="color:#d2a8ff;font-weight:bold">generate_text</span>(input_prompt: str) <span style="color:#ff7b72;font-weight:bold">-&gt;</span> str:
</span></span><span style="display:flex;"><span>    payload <span style="color:#ff7b72;font-weight:bold">=</span> {
</span></span><span style="display:flex;"><span>        <span style="color:#a5d6ff">&#34;inputs&#34;</span>: input_prompt,
</span></span><span style="display:flex;"><span>        <span style="color:#a5d6ff">&#34;min_length&#34;</span>: min_length,
</span></span><span style="display:flex;"><span>        <span style="color:#a5d6ff">&#34;max_length&#34;</span>: max_length,
</span></span><span style="display:flex;"><span>        <span style="color:#a5d6ff">&#34;temperature&#34;</span>: temperature,
</span></span><span style="display:flex;"><span>        <span style="color:#a5d6ff">&#34;repetition_penalty&#34;</span>: rep_penalty,
</span></span><span style="display:flex;"><span>        <span style="color:#a5d6ff">&#34;do_sample&#34;</span>: temperature <span style="color:#ff7b72;font-weight:bold">&gt;</span> <span style="color:#a5d6ff">0</span>,
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    response <span style="color:#ff7b72;font-weight:bold">=</span> sagemaker_runtime<span style="color:#ff7b72;font-weight:bold">.</span>invoke_endpoint(
</span></span><span style="display:flex;"><span>        EndpointName<span style="color:#ff7b72;font-weight:bold">=</span>endpoint_name,
</span></span><span style="display:flex;"><span>        ContentType<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;application/json&#34;</span>,
</span></span><span style="display:flex;"><span>        Body<span style="color:#ff7b72;font-weight:bold">=</span>json<span style="color:#ff7b72;font-weight:bold">.</span>dumps(payload)
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    result <span style="color:#ff7b72;font-weight:bold">=</span> json<span style="color:#ff7b72;font-weight:bold">.</span>loads(response[<span style="color:#a5d6ff">&#34;Body&#34;</span>]<span style="color:#ff7b72;font-weight:bold">.</span>read()<span style="color:#ff7b72;font-weight:bold">.</span>decode())
</span></span><span style="display:flex;"><span>    <span style="color:#ff7b72">return</span> result[<span style="color:#a5d6ff">0</span>][<span style="color:#a5d6ff">&#34;generated_text&#34;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>session <span style="color:#ff7b72;font-weight:bold">=</span> boto3<span style="color:#ff7b72;font-weight:bold">.</span>Session()
</span></span><span style="display:flex;"><span>sagemaker_runtime <span style="color:#ff7b72;font-weight:bold">=</span> session<span style="color:#ff7b72;font-weight:bold">.</span>client(<span style="color:#a5d6ff">&#34;sagemaker-runtime&#34;</span>, region_name<span style="color:#ff7b72;font-weight:bold">=</span>session<span style="color:#ff7b72;font-weight:bold">.</span>region_name)
</span></span><span style="display:flex;"><span>endpoint_name <span style="color:#ff7b72;font-weight:bold">=</span> <span style="color:#a5d6ff">&#34;dolly-v2-12b&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>st<span style="color:#ff7b72;font-weight:bold">.</span>sidebar<span style="color:#ff7b72;font-weight:bold">.</span>title(<span style="color:#a5d6ff">&#34;Dolly-V2 Parameters&#34;</span>)
</span></span><span style="display:flex;"><span>stop_word <span style="color:#ff7b72;font-weight:bold">=</span> st<span style="color:#ff7b72;font-weight:bold">.</span>sidebar<span style="color:#ff7b72;font-weight:bold">.</span>text_input(<span style="color:#a5d6ff">&#34;Stop word&#34;</span>)
</span></span><span style="display:flex;"><span>min_length, max_length <span style="color:#ff7b72;font-weight:bold">=</span> st<span style="color:#ff7b72;font-weight:bold">.</span>sidebar<span style="color:#ff7b72;font-weight:bold">.</span>slider(<span style="color:#a5d6ff">&#34;Min/Max length&#34;</span>, <span style="color:#a5d6ff">0</span>, <span style="color:#a5d6ff">500</span>, (<span style="color:#a5d6ff">0</span>, <span style="color:#a5d6ff">100</span>))
</span></span><span style="display:flex;"><span>temperature <span style="color:#ff7b72;font-weight:bold">=</span> st<span style="color:#ff7b72;font-weight:bold">.</span>sidebar<span style="color:#ff7b72;font-weight:bold">.</span>slider(<span style="color:#a5d6ff">&#34;Temperature&#34;</span>, min_value<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">0.0</span>, max_value<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">1.0</span>, value<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">0.6</span>)
</span></span><span style="display:flex;"><span>rep_penalty <span style="color:#ff7b72;font-weight:bold">=</span> st<span style="color:#ff7b72;font-weight:bold">.</span>sidebar<span style="color:#ff7b72;font-weight:bold">.</span>slider(<span style="color:#a5d6ff">&#34;Repetition Penalty&#34;</span>, min_value<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">0.9</span>, max_value<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">1.2</span>, value<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">1.0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>st<span style="color:#ff7b72;font-weight:bold">.</span>header(<span style="color:#a5d6ff">&#34;Dolly-v2-12B Playground&#34;</span>)
</span></span><span style="display:flex;"><span>prompt <span style="color:#ff7b72;font-weight:bold">=</span> st<span style="color:#ff7b72;font-weight:bold">.</span>text_area(<span style="color:#a5d6ff">&#34;Enter your prompt here:&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">if</span> st<span style="color:#ff7b72;font-weight:bold">.</span>button(<span style="color:#a5d6ff">&#34;Run&#34;</span>):
</span></span><span style="display:flex;"><span>    generated_text <span style="color:#ff7b72;font-weight:bold">=</span> generate_text(prompt)
</span></span><span style="display:flex;"><span>    <span style="color:#ff7b72">if</span> len(stop_word) <span style="color:#ff7b72;font-weight:bold">&gt;</span> <span style="color:#a5d6ff">0</span>:
</span></span><span style="display:flex;"><span>        generated_text <span style="color:#ff7b72;font-weight:bold">=</span> generated_text[:generated_text<span style="color:#ff7b72;font-weight:bold">.</span>rfind(stop_word)]
</span></span><span style="display:flex;"><span>    st<span style="color:#ff7b72;font-weight:bold">.</span>write(generated_text)</span></span></code></pre>
</figure><p>And finally execute the following command to deploy using docker:</p>
<figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>task run_playground</span></span></code></pre>
</figure><p><p class="md__image">
    <img src="images/playground-ui.png" alt=""  />
</p></p>
<p>You will be able to access the playground on: <code>http://localhost/8501/</code></p>
<h2 id="bonus-more-sophisticated-workflows">Bonus: More sophisticated workflows</h2>
<p>You can build need more sophisticated workflows by templating the prompts
using <a href="https://python.langchain.com/en/latest/">langchain</a>. The following are just a few examples of what you can do by
combining <code>langchain</code> and <code>SageMaker</code>:</p>
<figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>!pip install langchain
</span></span><span style="display:flex;"><span>!aws configure set aws_access_key_id <span style="color:#ff7b72;font-weight:bold">[</span>REDACTED<span style="color:#ff7b72;font-weight:bold">]</span>
</span></span><span style="display:flex;"><span>!aws configure set aws_secret_access_key <span style="color:#ff7b72;font-weight:bold">[</span>REDACTED<span style="color:#ff7b72;font-weight:bold">]</span>
</span></span><span style="display:flex;"><span>!aws configure set default.region us-east-1</span></span></code></pre>
</figure><figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff7b72">import</span> <span style="color:#ff7b72">json</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">from</span> <span style="color:#ff7b72">langchain</span> <span style="color:#ff7b72">import</span> SagemakerEndpoint
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">from</span> <span style="color:#ff7b72">langchain.llms.sagemaker_endpoint</span> <span style="color:#ff7b72">import</span> LLMContentHandler
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">class</span> <span style="color:#f0883e;font-weight:bold">ContentHandler</span>(LLMContentHandler):
</span></span><span style="display:flex;"><span>    content_type <span style="color:#ff7b72;font-weight:bold">=</span> <span style="color:#a5d6ff">&#34;application/json&#34;</span>
</span></span><span style="display:flex;"><span>    accepts <span style="color:#ff7b72;font-weight:bold">=</span> <span style="color:#a5d6ff">&#34;application/json&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#ff7b72">def</span> <span style="color:#d2a8ff;font-weight:bold">transform_input</span>(self, prompt: str, model_kwargs) <span style="color:#ff7b72;font-weight:bold">-&gt;</span> bytes:
</span></span><span style="display:flex;"><span>        input_str <span style="color:#ff7b72;font-weight:bold">=</span> json<span style="color:#ff7b72;font-weight:bold">.</span>dumps({prompt: prompt, <span style="color:#ff7b72;font-weight:bold">**</span>model_kwargs})
</span></span><span style="display:flex;"><span>        <span style="color:#ff7b72">return</span> input_str<span style="color:#ff7b72;font-weight:bold">.</span>encode(<span style="color:#a5d6ff">&#39;utf-8&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#ff7b72">def</span> <span style="color:#d2a8ff;font-weight:bold">transform_output</span>(self, output: bytes) <span style="color:#ff7b72;font-weight:bold">-&gt;</span> str:
</span></span><span style="display:flex;"><span>        response_json <span style="color:#ff7b72;font-weight:bold">=</span> json<span style="color:#ff7b72;font-weight:bold">.</span>loads(output<span style="color:#ff7b72;font-weight:bold">.</span>read()<span style="color:#ff7b72;font-weight:bold">.</span>decode(<span style="color:#a5d6ff">&#34;utf-8&#34;</span>))
</span></span><span style="display:flex;"><span>        <span style="color:#ff7b72">return</span> response_json[<span style="color:#a5d6ff">0</span>][<span style="color:#a5d6ff">&#34;generated_text&#34;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>se <span style="color:#ff7b72;font-weight:bold">=</span> SagemakerEndpoint(
</span></span><span style="display:flex;"><span>    endpoint_name<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;dolly-v2-12b&#34;</span>,
</span></span><span style="display:flex;"><span>    region_name<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;us-east-1&#34;</span>,
</span></span><span style="display:flex;"><span>    credentials_profile_name<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;default&#34;</span>,
</span></span><span style="display:flex;"><span>    content_handler<span style="color:#ff7b72;font-weight:bold">=</span>ContentHandler(),
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>se(<span style="color:#a5d6ff">&#34;Tell me a joke&#34;</span>)</span></span></code></pre>
</figure><figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff7b72">from</span> <span style="color:#ff7b72">langchain</span> <span style="color:#ff7b72">import</span> PromptTemplate, LLMChain
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>prompt_template <span style="color:#ff7b72;font-weight:bold">=</span> <span style="color:#a5d6ff">&#34;Why is </span><span style="color:#a5d6ff">{vegetable}</span><span style="color:#a5d6ff"> good for you?&#34;</span>
</span></span><span style="display:flex;"><span>llm_chain <span style="color:#ff7b72;font-weight:bold">=</span> LLMChain(llm<span style="color:#ff7b72;font-weight:bold">=</span>se, prompt<span style="color:#ff7b72;font-weight:bold">=</span>PromptTemplate<span style="color:#ff7b72;font-weight:bold">.</span>from_template(prompt_template))
</span></span><span style="display:flex;"><span>llm_chain(<span style="color:#a5d6ff">&#34;brocolli&#34;</span>)[<span style="color:#a5d6ff">&#34;text&#34;</span>]</span></span></code></pre>
</figure><figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff7b72">import</span> <span style="color:#ff7b72">time</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">from</span> <span style="color:#ff7b72">langchain.chains</span> <span style="color:#ff7b72">import</span> LLMChain
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">from</span> <span style="color:#ff7b72">langchain.prompts</span> <span style="color:#ff7b72">import</span> PromptTemplate
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">def</span> <span style="color:#d2a8ff;font-weight:bold">generate_serially</span>():
</span></span><span style="display:flex;"><span>    prompt <span style="color:#ff7b72;font-weight:bold">=</span> PromptTemplate(
</span></span><span style="display:flex;"><span>        input_variables<span style="color:#ff7b72;font-weight:bold">=</span>[<span style="color:#a5d6ff">&#34;product&#34;</span>],
</span></span><span style="display:flex;"><span>        template<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;What is a good name for a company that makes </span><span style="color:#a5d6ff">{product}</span><span style="color:#a5d6ff">?&#34;</span>,
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    chain <span style="color:#ff7b72;font-weight:bold">=</span> LLMChain(llm<span style="color:#ff7b72;font-weight:bold">=</span>se, prompt<span style="color:#ff7b72;font-weight:bold">=</span>prompt)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">for</span> _ <span style="color:#ff7b72;font-weight:bold">in</span> range(<span style="color:#a5d6ff">5</span>):
</span></span><span style="display:flex;"><span>    resp <span style="color:#ff7b72;font-weight:bold">=</span> chain<span style="color:#ff7b72;font-weight:bold">.</span>run(product<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;underwear&#34;</span>)
</span></span><span style="display:flex;"><span>    print(resp)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>s <span style="color:#ff7b72;font-weight:bold">=</span> time<span style="color:#ff7b72;font-weight:bold">.</span>perf_counter()
</span></span><span style="display:flex;"><span>generate_serially()
</span></span><span style="display:flex;"><span>elapsed <span style="color:#ff7b72;font-weight:bold">=</span> time<span style="color:#ff7b72;font-weight:bold">.</span>perf_counter() <span style="color:#ff7b72;font-weight:bold">-</span> s
</span></span><span style="display:flex;"><span>print(<span style="color:#a5d6ff">&#39;</span><span style="color:#79c0ff">\033</span><span style="color:#a5d6ff">[1m&#39;</span> <span style="color:#ff7b72;font-weight:bold">+</span> <span style="color:#79c0ff">f</span><span style="color:#a5d6ff">&#34;Serial executed in </span><span style="color:#a5d6ff">{</span>elapsed<span style="color:#a5d6ff">:</span><span style="color:#a5d6ff">0.2f</span><span style="color:#a5d6ff">}</span><span style="color:#a5d6ff"> seconds.&#34;</span> <span style="color:#ff7b72;font-weight:bold">+</span> <span style="color:#a5d6ff">&#39;</span><span style="color:#79c0ff">\033</span><span style="color:#a5d6ff">[0m&#39;</span>)</span></span></code></pre>
</figure><figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-python" data-lang="python"><span style="display:flex;"><span>prompt_template <span style="color:#ff7b72;font-weight:bold">=</span> <span style="color:#a5d6ff">&#34;Tell me a </span><span style="color:#a5d6ff">{adjective}</span><span style="color:#a5d6ff"> joke&#34;</span>
</span></span><span style="display:flex;"><span>llm_chain <span style="color:#ff7b72;font-weight:bold">=</span> LLMChain(
</span></span><span style="display:flex;"><span>   llm<span style="color:#ff7b72;font-weight:bold">=</span>se,
</span></span><span style="display:flex;"><span>   prompt<span style="color:#ff7b72;font-weight:bold">=</span>PromptTemplate<span style="color:#ff7b72;font-weight:bold">.</span>from_template(prompt_template)
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>llm_chain(inputs<span style="color:#ff7b72;font-weight:bold">=</span>{<span style="color:#a5d6ff">&#34;adjective&#34;</span>: <span style="color:#a5d6ff">&#34;corny&#34;</span>})</span></span></code></pre>
</figure><h3 id="cleaning-up-resources">Cleaning up resources</h3>
<p>To clean up the resources created by this project, you can run the following command:</p>
<figure class="highlight">
    <pre tabindex="0" class="chroma"
    ><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>task tf_destroy</span></span></code></pre>
</figure>
			</div>

			<div class="tags">
				
					
						<ul class="flat">
							
							<li><a href="/tags/mlops">mlops</a></li>
							
							<li><a href="/tags/llm">llm</a></li>
							
							<li><a href="/tags/dolly">dolly</a></li>
							
							<li><a href="/tags/langchain">langchain</a></li>
							
							<li><a href="/tags/sagemaker">sagemaker</a></li>
							
							<li><a href="/tags/hugging-face">hugging face</a></li>
							
							<li><a href="/tags/aws">aws</a></li>
							
							<li><a href="/tags/databricks">databricks</a></li>
							
							<li><a href="/tags/activeloop">activeloop</a></li>
							
						</ul>
					
				
			</div><l></l><div id="disqus_thread"></div>
<script>

    (function() {
        
        
        if (window.location.hostname == "localhost")
            return;

        var d = document, s = d.createElement('script');
        s.src = 'https://mariano-gonzalez.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
	</div>
	<div class="footer wrapper">
	<nav class="nav">
		<div>2023  <p>Copyright © 2023 Mariano Gonzalez</p><p xmlns:dct='http://purl.org/dc/terms/' xmlns:cc='http://creativecommons.org/ns#' class='license-text'><a rel='cc:attributionURL' property='dct:title' href='https://github.com/eschizoid/eschizoid.github.io/tree/main/content'>Content</a> licensed under <a rel='license' href='https://creativecommons.org/licenses/by/4.0'>CC BY 4.0</a></p> |  <a href="https://github.com/knadh/hugo-ink">Ink</a> theme on <a href="https://gohugo.io">Hugo</a></div>
	</nav>
</div>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-1R50XCY25D"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-1R50XCY25D');
</script><script>feather.replace()</script>
</body>
</html>
